{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "하다보면 늘겠지 : 14. Language model with LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "metadata": {
        "id": "JHauSoYaMEiJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKlTFj6PTSEO",
        "outputId": "f7a672bf-ff85-4ab8-ff78-3b4e8d020c1e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "embed_size = 128\n",
        "hidden_size = 128\n",
        "num_layers = 1\n",
        "num_epochs = 10\n",
        "num_samples = 1000  # number of words to be sampled\n",
        "batch_size = 20\n",
        "seq_length = 30\n",
        "lr = 5e-3"
      ],
      "metadata": {
        "id": "n_iYPQhZTfVL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary(object): # Actually, starting with python 3.x version, we don't need to inherit object class\n",
        "    def __init__(self):\n",
        "        self.word2idx = {} # key : word, val : idx\n",
        "        self.idx2word = {} # key : idx, val : word\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "    \n",
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "\n",
        "    def get_data(self, path, batch_size = 20):\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>'] # ['blah1', 'blah2', ..., '<eos>'] \n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "        \n",
        "        # Tokenize the file content\n",
        "        ids = torch.LongTensor(tokens) # create tensor of tokens initialized with random number // try torch.LongTensor(3)\n",
        "        token = 0\n",
        "        with open(path, 'r') as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "        num_batches = ids.size(0) // batch_size\n",
        "        ids = ids[:num_batches*batch_size]\n",
        "        return ids.view(batch_size, -1)"
      ],
      "metadata": {
        "id": "H21W3YTbN2Wc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load \"Penn Treebank\" dataset\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data('data/train.txt', batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "num_batches = ids.size(1) // seq_length"
      ],
      "metadata": {
        "id": "A0cK8hY5TSBj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN based language model\n",
        "class RNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, x, h):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, (h,c) = self.lstm(x, h)\n",
        "\n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "\n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        return out, (h, c)"
      ],
      "metadata": {
        "id": "HSy0SKHEU0hC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)"
      ],
      "metadata": {
        "id": "1eWxw03LU0e0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "8Pp1ikKvU0ci"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncated backpropagation\n",
        "def detach(states):\n",
        "    return [state.detach() for state in states]\n",
        "    # See the train code below"
      ],
      "metadata": {
        "id": "UJpMzKxrU0aD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "log_interval = 300\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "    \n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+seq_length].to(device)\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "\n",
        "        # Forward\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % log_interval == 0:\n",
        "            print(f'Epoch [{epoch}/{num_epochs}], Step [{step}/{num_batches}, Loss : {loss.item():.4f}, Perplexity : {np.exp(loss.item()):5.2f}]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJHTuH94U0Xi",
        "outputId": "915e71cd-5076-4bb2-a566-bf11066403b3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [0/1549, Loss : 9.2157, Perplexity : 10053.94]\n",
            "Epoch [1/10], Step [300/1549, Loss : 5.8653, Perplexity : 352.58]\n",
            "Epoch [1/10], Step [600/1549, Loss : 5.3524, Perplexity : 211.11]\n",
            "Epoch [1/10], Step [900/1549, Loss : 5.2562, Perplexity : 191.74]\n",
            "Epoch [1/10], Step [1200/1549, Loss : 5.3735, Perplexity : 215.62]\n",
            "Epoch [1/10], Step [1500/1549, Loss : 5.3487, Perplexity : 210.34]\n",
            "Epoch [2/10], Step [0/1549, Loss : 5.6463, Perplexity : 283.25]\n",
            "Epoch [2/10], Step [300/1549, Loss : 5.0350, Perplexity : 153.70]\n",
            "Epoch [2/10], Step [600/1549, Loss : 4.8439, Perplexity : 126.96]\n",
            "Epoch [2/10], Step [900/1549, Loss : 4.7450, Perplexity : 115.00]\n",
            "Epoch [2/10], Step [1200/1549, Loss : 4.9996, Perplexity : 148.36]\n",
            "Epoch [2/10], Step [1500/1549, Loss : 4.9720, Perplexity : 144.31]\n",
            "Epoch [3/10], Step [0/1549, Loss : 5.1317, Perplexity : 169.30]\n",
            "Epoch [3/10], Step [300/1549, Loss : 4.6847, Perplexity : 108.28]\n",
            "Epoch [3/10], Step [600/1549, Loss : 4.6320, Perplexity : 102.72]\n",
            "Epoch [3/10], Step [900/1549, Loss : 4.4796, Perplexity : 88.20]\n",
            "Epoch [3/10], Step [1200/1549, Loss : 4.7591, Perplexity : 116.64]\n",
            "Epoch [3/10], Step [1500/1549, Loss : 4.7092, Perplexity : 110.97]\n",
            "Epoch [4/10], Step [0/1549, Loss : 4.7873, Perplexity : 119.98]\n",
            "Epoch [4/10], Step [300/1549, Loss : 4.5312, Perplexity : 92.87]\n",
            "Epoch [4/10], Step [600/1549, Loss : 4.4518, Perplexity : 85.78]\n",
            "Epoch [4/10], Step [900/1549, Loss : 4.3140, Perplexity : 74.74]\n",
            "Epoch [4/10], Step [1200/1549, Loss : 4.6226, Perplexity : 101.76]\n",
            "Epoch [4/10], Step [1500/1549, Loss : 4.5558, Perplexity : 95.19]\n",
            "Epoch [5/10], Step [0/1549, Loss : 4.5439, Perplexity : 94.06]\n",
            "Epoch [5/10], Step [300/1549, Loss : 4.4043, Perplexity : 81.80]\n",
            "Epoch [5/10], Step [600/1549, Loss : 4.3637, Perplexity : 78.55]\n",
            "Epoch [5/10], Step [900/1549, Loss : 4.1664, Perplexity : 64.48]\n",
            "Epoch [5/10], Step [1200/1549, Loss : 4.5092, Perplexity : 90.85]\n",
            "Epoch [5/10], Step [1500/1549, Loss : 4.4524, Perplexity : 85.83]\n",
            "Epoch [6/10], Step [0/1549, Loss : 4.3521, Perplexity : 77.64]\n",
            "Epoch [6/10], Step [300/1549, Loss : 4.2775, Perplexity : 72.06]\n",
            "Epoch [6/10], Step [600/1549, Loss : 4.3112, Perplexity : 74.53]\n",
            "Epoch [6/10], Step [900/1549, Loss : 4.0648, Perplexity : 58.25]\n",
            "Epoch [6/10], Step [1200/1549, Loss : 4.4204, Perplexity : 83.13]\n",
            "Epoch [6/10], Step [1500/1549, Loss : 4.3246, Perplexity : 75.54]\n",
            "Epoch [7/10], Step [0/1549, Loss : 4.2356, Perplexity : 69.10]\n",
            "Epoch [7/10], Step [300/1549, Loss : 4.2139, Perplexity : 67.62]\n",
            "Epoch [7/10], Step [600/1549, Loss : 4.2747, Perplexity : 71.86]\n",
            "Epoch [7/10], Step [900/1549, Loss : 3.9952, Perplexity : 54.34]\n",
            "Epoch [7/10], Step [1200/1549, Loss : 4.3518, Perplexity : 77.62]\n",
            "Epoch [7/10], Step [1500/1549, Loss : 4.2215, Perplexity : 68.13]\n",
            "Epoch [8/10], Step [0/1549, Loss : 4.2027, Perplexity : 66.87]\n",
            "Epoch [8/10], Step [300/1549, Loss : 4.1518, Perplexity : 63.55]\n",
            "Epoch [8/10], Step [600/1549, Loss : 4.2045, Perplexity : 66.99]\n",
            "Epoch [8/10], Step [900/1549, Loss : 3.9212, Perplexity : 50.46]\n",
            "Epoch [8/10], Step [1200/1549, Loss : 4.3051, Perplexity : 74.08]\n",
            "Epoch [8/10], Step [1500/1549, Loss : 4.1392, Perplexity : 62.75]\n",
            "Epoch [9/10], Step [0/1549, Loss : 4.1254, Perplexity : 61.89]\n",
            "Epoch [9/10], Step [300/1549, Loss : 4.0773, Perplexity : 58.98]\n",
            "Epoch [9/10], Step [600/1549, Loss : 4.1694, Perplexity : 64.67]\n",
            "Epoch [9/10], Step [900/1549, Loss : 3.8905, Perplexity : 48.94]\n",
            "Epoch [9/10], Step [1200/1549, Loss : 4.2384, Perplexity : 69.30]\n",
            "Epoch [9/10], Step [1500/1549, Loss : 4.0880, Perplexity : 59.62]\n",
            "Epoch [10/10], Step [0/1549, Loss : 4.0665, Perplexity : 58.35]\n",
            "Epoch [10/10], Step [300/1549, Loss : 4.0263, Perplexity : 56.06]\n",
            "Epoch [10/10], Step [600/1549, Loss : 4.1037, Perplexity : 60.56]\n",
            "Epoch [10/10], Step [900/1549, Loss : 3.8011, Perplexity : 44.75]\n",
            "Epoch [10/10], Step [1200/1549, Loss : 4.1851, Perplexity : 65.70]\n",
            "Epoch [10/10], Step [1500/1549, Loss : 4.0103, Perplexity : 55.16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    with open('sample.txt', 'w') as f:\n",
        "        # Set initial hidden and cell states\n",
        "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "        \n",
        "        # Select one word id randomly\n",
        "        prob = torch.ones(vocab_size)\n",
        "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device) # return idx\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Forward\n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "            \n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = corpus.dictionary.idx2word[word_id]\n",
        "            word = '\\n' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1)%100 == 0:\n",
        "                print(f'Sampled [{i+1}/{num_samples}] words and save to {\"sample.txt\"}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2_TwmyRZ6VG",
        "outputId": "301296f9-1a19-4faf-8301-be71591aa08d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled [100/1000] words and save to sample.txt\n",
            "Sampled [200/1000] words and save to sample.txt\n",
            "Sampled [300/1000] words and save to sample.txt\n",
            "Sampled [400/1000] words and save to sample.txt\n",
            "Sampled [500/1000] words and save to sample.txt\n",
            "Sampled [600/1000] words and save to sample.txt\n",
            "Sampled [700/1000] words and save to sample.txt\n",
            "Sampled [800/1000] words and save to sample.txt\n",
            "Sampled [900/1000] words and save to sample.txt\n",
            "Sampled [1000/1000] words and save to sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "id": "M2ldDZ_HZ6Sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
