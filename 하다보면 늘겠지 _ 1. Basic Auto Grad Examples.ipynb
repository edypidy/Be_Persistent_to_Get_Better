{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"하다보면 늘겠지 : 1. Basic Auto Grad Examples.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNnoXvtgngM61CVLSRq/Urf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xfXTZXfKkchr"},"outputs":[],"source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.transforms as transforms"]},{"cell_type":"markdown","source":["# 1. Basic autograd examples"],"metadata":{"id":"62xtGjNMlUVL"}},{"cell_type":"code","source":["# Example1.\n","\n","# Create Tensors\n","# 1.과 같이 .을 붙이는 이유는 텐서에 들어가는 포인트가 플롯형이어야 하기 때문입니다.\n","x = torch.tensor(1., requires_grad = True)\n","w = torch.tensor(2., requires_grad = True)\n","b = torch.tensor(3., requires_grad = True)"],"metadata":{"id":"6gCUEYTHlTKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build a Computational Graph\n","y = x*w + b\n","\n","# Compute Gradients.\n","y.backward()\n","\n","print(x.grad)\n","print(w.grad)\n","print(b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTpRH3wYl8gW","executionInfo":{"status":"ok","timestamp":1643590067029,"user_tz":-540,"elapsed":2,"user":{"displayName":"이디피디","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17709352578234406943"}},"outputId":"2f196c04-d710-4f4f-b9cc-81b34501b327"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.)\n","tensor(1.)\n","tensor(1.)\n"]}]},{"cell_type":"code","source":["# grad는 tensor의 grad에 누적됩니다.\n","\n","x = torch.tensor(1., requires_grad = True)\n","w = torch.tensor(2., requires_grad = True)\n","b = torch.tensor(3., requires_grad = True)\n","\n","for _ in range(5):\n","    y = x*w + b\n","    y.backward()\n","    print('x\\'s grad is accumulated:', x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9yofWHfmgQ2","executionInfo":{"status":"ok","timestamp":1643590331187,"user_tz":-540,"elapsed":267,"user":{"displayName":"이디피디","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17709352578234406943"}},"outputId":"3612aba5-550a-41f3-b1c4-bb6ca612c224"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x's grad is accumulated: tensor(2.)\n","x's grad is accumulated: tensor(4.)\n","x's grad is accumulated: tensor(6.)\n","x's grad is accumulated: tensor(8.)\n","x's grad is accumulated: tensor(10.)\n"]}]},{"cell_type":"code","source":["# Example2.\n","\n","# Create Tensors of shape (10,3) and (10, 2)\n","x = torch.ones(10,3) # 초기값을 1로 주어 computation이 어떻게 이뤄지는지 알아봅시다.\n","y = torch.ones(10,2)\n","print('x: ', x)\n","\n","# Build a fully connected layer.\n","linear = nn.Linear(3, 2)\n","print('w: ', linear.weight)\n","print('b: ', linear.bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VgAf5Ywynsay","executionInfo":{"status":"ok","timestamp":1643591793570,"user_tz":-540,"elapsed":259,"user":{"displayName":"이디피디","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17709352578234406943"}},"outputId":"2cb8d899-4395-4f74-9421-ad4220cc7f31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x:  tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","w:  Parameter containing:\n","tensor([[ 0.3120,  0.1399, -0.4323],\n","        [-0.3648,  0.0290, -0.5620]], requires_grad=True)\n","b:  Parameter containing:\n","tensor([-0.4878, -0.1206], requires_grad=True)\n"]}]},{"cell_type":"code","source":["# Build loss function and optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n","\n","# Forward pass.\n","pred = linear(x)"],"metadata":{"id":"iMwyNe_Gog6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred\n","# 10행 3열의 x에 2행 3열인 w를 transpose해서 행렬곱을 하고 bias를 더합니다.\n","# 실제 계산에서는 x에 합벡터를 concat하고 w와 bias를 concat해서 서로 행렬곱을 하는 식이 되겠네요."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GmMlZuYko3zQ","executionInfo":{"status":"ok","timestamp":1643591795854,"user_tz":-540,"elapsed":3,"user":{"displayName":"이디피디","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17709352578234406943"}},"outputId":"ede1cd69-66ca-485c-c7e8-3b501d244a7e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.4683, -1.0185],\n","        [-0.4683, -1.0185],\n","        [-0.4683, -1.0185],\n","        [-0.4683, -1.0185],\n","        [-0.4683, -1.0185],\n","        [-0.4683, -1.0185],\n","        [-0.4683, -1.0185],\n","        [-0.4683, -1.0185],\n","        [-0.4683, -1.0185],\n","        [-0.4683, -1.0185]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# Compute Loss\n","loss = criterion(pred, y)\n","print('loss: ', loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mI4bZdekstfc","executionInfo":{"status":"ok","timestamp":1643591796801,"user_tz":-540,"elapsed":6,"user":{"displayName":"이디피디","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17709352578234406943"}},"outputId":"22c515fe-d3d2-4f93-fc97-23f96c525142"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loss:  3.1151137351989746\n"]}]},{"cell_type":"code","source":["# Backward pass.\n","loss.backward()\n","\n","# Print out the gradients.\n","print('dL/dw: ', linear.weight.grad)\n","print('dL/db: ', linear.bias.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kd159Pato4AO","executionInfo":{"status":"ok","timestamp":1643591941822,"user_tz":-540,"elapsed":262,"user":{"displayName":"이디피디","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17709352578234406943"}},"outputId":"835eef79-4cd4-4fee-c161-d62f408e06f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dL/dw:  tensor([[-1.4683, -1.4683, -1.4683],\n","        [-2.0185, -2.0185, -2.0185]])\n","dL/db:  tensor([-1.4683, -2.0185])\n"]}]},{"cell_type":"code","source":["# 1-step gradient descent.\n","optimizer.step()"],"metadata":{"id":"Z4hRmQSGt2mo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # optimizer.step()을 low level로 수행하면 아래와 같습니다.\n","# # sub인 이유는 최적화시 gradient에 -를 해서 내려가는 식으로 작동하기 때문입니다.\n","# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n","# linear.bias.data.sub_(0.01 * linear.bias.grad.data)"],"metadata":{"id":"O-4AiKMguK7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print out the loss after 1-step gradient descent.\n","pred = linear(x)\n","loss = criterion(pred, y)\n","print('loss after 1-step gradient descent : ', loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3rT6kcUug2i","executionInfo":{"status":"ok","timestamp":1643592255716,"user_tz":-540,"elapsed":268,"user":{"displayName":"이디피디","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17709352578234406943"}},"outputId":"6f906b4f-5784-48fb-cb88-6ab087d2b943"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loss after 1-step gradient descent :  2.8708889484405518\n"]}]},{"cell_type":"code","source":["for i in range(0, 3):\n","    print(f'{i} epoch:' + '='*30)\n","\n","    pred = linear(x)\n","    loss = criterion(pred, y) # computational graph가 만들어지고\n","    print(f'loss after {i}-step gradient descent : ', loss.item())\n","\n","    loss.backward() # 만들어진 computational graph에서 auto_grad로 계산된 grad가 linear로 전파됩니다.\n","    print('dL/dw: ', linear.weight.grad)\n","    print('dL/db: ', linear.bias.grad)\n","    \n","    optimizer.zero_grad() #해주지 않으면 그레디언트가 계속 누적되어 이상한 결과를 낸다.\n","    optimizer.step() # linear의 parameters()에서 전파되었던 grad로 SGD가 최적화하여 weight를 업데이트 시킵니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A29Y3Efsu9lX","executionInfo":{"status":"ok","timestamp":1643592569650,"user_tz":-540,"elapsed":265,"user":{"displayName":"이디피디","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17709352578234406943"}},"outputId":"25c8abff-df62-4883-d356-7c1aac216713"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 epoch:==============================\n","loss after 0-step gradient descent :  2.8708889484405518\n","dL/dw:  tensor([[-2.8778, -2.8778, -2.8778],\n","        [-3.9563, -3.9563, -3.9563]])\n","dL/db:  tensor([-2.8778, -3.9563])\n","1 epoch:==============================\n","loss after 1-step gradient descent :  2.421124219894409\n","dL/dw:  tensor([[-4.1722, -4.1722, -4.1722],\n","        [-5.7358, -5.7358, -5.7358]])\n","dL/db:  tensor([-4.1722, -5.7358])\n","2 epoch:==============================\n","loss after 2-step gradient descent :  1.8370624780654907\n","dL/dw:  tensor([[-5.2998, -5.2998, -5.2998],\n","        [-7.2859, -7.2859, -7.2859]])\n","dL/db:  tensor([-5.2998, -7.2859])\n"]}]},{"cell_type":"markdown","source":["# 2. Loading Data from numpy"],"metadata":{"id":"m6qZ0gDtBlXj"}},{"cell_type":"code","source":["# Create a numpy array.\n","x = np.array([[1, 2],\n","              [3,4]])\n","\n","# Convert the numpy array to a torch tensor.\n","y = torch.from_numpy(x)\n","\n","# Convert the torch tensor to a numpy array.\n","z = y.numpy()"],"metadata":{"id":"7pURtc7gBrgP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Input Pipeline"],"metadata":{"id":"6mWAUT5ECMRV"}},{"cell_type":"code","source":["# Download and construct CIFAR-10 dataset.\n","train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n","                                             train=True,\n","                                             transform=transforms.ToTensor(),\n","                                             download=True,\n","                                             )\n","\n","# Fetch one data pair (read data from disk)\n","image, label = train_dataset[0]\n","print(image.size())\n","print(label)"],"metadata":{"id":"BlgFKZrcBrko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Loader (this provides queues and threads in a very simple way).\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=32,\n","                                           shuffle=True,\n","                                           )"],"metadata":{"id":"Osw-vpduBrof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# When iteration starts, queue and thread start to load data from files.\n","data_iter = iter(train_loader)\n","\n","# Mini-batch images and labels.\n","images, labels = data_iter.next()\n","\n","# Actual usage of the data loader is as below\n","for images, labels in train_loader:\n","    # Train코드가 이 자리에 들어와야 합니다.\n","    pass"],"metadata":{"id":"dwPv_LqFBrsO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Input pipeline for Custom Dataset\n","\n","import pandas as pd\n","import os\n","from torchvision.io import read_image\n","\n","to_tensor = ToTensor()\n","\n","# Build Custom dataset Class\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, img_dir, annot_file, transform=None, target_transform=None):\n","        self.img_dir = img_dir\n","        self.img_labels = pd.read_csv(annot_file, names=['file_name', 'target'])\n","        self.transform = transform\n","        self.target_transform = target_transform\n","    \n","    def __len__(self):\n","        return len(self.img_dir)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            transform_target = self.target_transform(label)\n","            label = transform_target\n","        \n","        image = to_tensor(image)\n","        target = torch.as_tensor(label)\n","\n","        return image, target"],"metadata":{"id":"5wRMzbN7D-DU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Plus\n","import cv2\n","\n","def generate_target(target):\n","    pass\n","\n","class CustomAdvancedDataset(torch.utils.data.Dataset):\n","    def __init__(self, img_dir, annot_dir, transform=None, target_transform=None):\n","        self.img_dir = img_dir\n","        self.imgs = list(sorted(os.listdir(self.img_dir)))\n","        self.annot_dir = annot_dir\n","        self.annots = list(sorted(os.listdir(self.annot_dir)))\n","\n","        self.transform = transform\n","        self.target_transform = target_transform\n","    \n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        file_image = self.imgs[idx]\n","        file_target = self.annots[idx]\n","\n","        img_path = os.path.join(self.img_dir, file_image)\n","        target_path = os.path.join(self.annot_dir, file_target)\n","\n","        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","        target = generate_target(target_path)\n","\n","        if self.transform:\n","            img = self.transform(img)\n","        if self.target_transform:\n","            transform_target = self.transform_target(target)\n","            target = transform_target\n","        \n","        img = to_tensor(img)\n","        target = torch.as_tensor(target)\n","\n","        return img, target"],"metadata":{"id":"uW0mBwknHhKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_dataset = CustomDataset()\n","train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n","                                           batch_size=32,\n","                                           shuffle=True,\n","                                           )"],"metadata":{"id":"R3J5vttfUiIw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Pretrained Model"],"metadata":{"id":"2fLZP4fgUfGj"}},{"cell_type":"code","source":["# Download and load the pretrained ResNet-18\n","resnet = torchvision.models.resnet18(pretrained=True)\n","\n","# If you want to finetuning only the top layer of the model, set as below.\n","for param in resnet.parameters():\n","    param.requires_grad = False # torch.tensor의 requires_grad attr이 False가 되면서 업데이트를 하지 않게 됩니다.\n","\n","# Replace the top layer for finetuning\n","resnet.fc = nn.Linear(resnet.fc.in_features, 100) \n","# 최상단의 레이어는 fully connected layer인데 100은 그냥 넣어본 숫자이고 output으로 나갈 수 있게끔 차원을 조정하면 됩니다.\n","\n","# Forward pass.\n","images = torch.randn(64,3,224, 224)\n","outputs = resnet(images)\n","print(outputs.size())"],"metadata":{"id":"f5q45LnAUwAR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Save and load the model"],"metadata":{"id":"pUpibuJAWOD_"}},{"cell_type":"code","source":["# Save and load the entire model.\n","## Save\n","torch.save(resnet, 'model.ckpt')\n","## Load\n","model = torch.load('model.ckpt')\n","\n","# Save and load only the model parameters (recommended)\n","## Save\n","torch.save(resnet.state_dict(), 'params.ckpt')\n","\n","## Load\n","resnet.load_state_dict(torch.load('params.ckpt'))"],"metadata":{"id":"r5regBxxWMP_"},"execution_count":null,"outputs":[]}]}